{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dea12218-b235-4ae9-95eb-3195a2aa5bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH (n) RETURN COUNT(n)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT(n)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   COUNT(n)\n",
       "0     94193"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import openai\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import openai\n",
    "#import aws secrets manager infrastructure\n",
    "import boto3 \n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neo4j\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "import urllib.parse\n",
    "import os,sys\n",
    "\n",
    "def get_aws_secret_pws(pw_to_find):\n",
    "\n",
    "    secret_name = \"omealerts_pws/{}\".format(pw_to_find)\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    # We rethrow the exception by default.\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "        return get_secret_value_response\n",
    "    except ClientError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "currentdir = os.path.dirname(os.path.realpath('Jupyterlab/Ankur_Notebooks/Sumi_KG/Neo4j_DS.ipynb'))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.append(parentdir)\n",
    "sys.path.append(currentdir)\n",
    "\n",
    "omealerts_kg_access_username = 'omealerts_kg_access'\n",
    "omealerts_kg_access_pw = json.loads(get_aws_secret_pws('omealerts_kg_access').get('SecretString', False)).get('omealerts_kg_access', None)\n",
    "\n",
    "host = \"bolt://10.115.1.170:7687\"\n",
    "user = omealerts_kg_access_username\n",
    "password = str(omealerts_kg_access_pw)\n",
    "database='ome-alerts'\n",
    "driver = GraphDatabase.driver(host,auth=(user, password))\n",
    "db =driver.session(database=database)\n",
    "\n",
    "openai.api_key = json.loads(get_aws_secret_pws('openai_api_key').get('SecretString'))['omealerts_pws/openai_api_key']\n",
    "MODEL=\"gpt-4\"\n",
    "\n",
    "def unpack_res(r):\n",
    "    res =  r['choices'][0]['message']['content']\n",
    "    tokens = r['usage']['total_tokens']\n",
    "    response = r\n",
    "    return res,tokens,response\n",
    "\n",
    "def run_query_df(query, params={}):\n",
    "    with driver.session(database=database) as session:\n",
    "        result = session.run(query, params)\n",
    "        data = [record.data() for record in result]\n",
    "        df = pd.json_normalize(data)\n",
    "        return df\n",
    "\n",
    "\n",
    "##Read cypher query results into Dataframe\n",
    "def run_query(query):\n",
    "        with driver.session(database=database) as session:\n",
    "            result = session.run(query)\n",
    "            print(query)\n",
    "            return pd.DataFrame([r.values() for r in result], columns=result.keys())\n",
    "\n",
    "\n",
    "# foo = run_query_df(\"\"\"MATCH (n) RETURN (n) LIMIT 5\"\"\")\n",
    "from neo4j import GraphDatabase\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as mcm\n",
    "from neo4j import GraphDatabase\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "run_query(\"MATCH (n) RETURN COUNT(n)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f0a76-f080-4d8c-a66c-4dc83d17dd53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# foo = run_query_df(\"\"\"MATCH (n) RETURN (n) LIMIT 5\"\"\")\n",
    "from neo4j import GraphDatabase\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as mcm\n",
    "from neo4j import GraphDatabase\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from py2neo import Graph\n",
    "class Neo4jHeteroGraphStore:\n",
    "    def __init__(self, uri, user, password, database, data_dir='data', use_file_storage=True):\n",
    "        self.graph = Graph(uri, name=database, auth=(user, password))\n",
    "        self.data_dir = data_dir\n",
    "        self.use_file_storage = use_file_storage\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            os.makedirs(self.data_dir)\n",
    "    \n",
    "    def _save_data(self, data, filename):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(self.data_dir, filename), 'w') as f:\n",
    "                json.dump(data, f)\n",
    "    \n",
    "    def _load_data(self, filename):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(self.data_dir, filename)) as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "    \n",
    "    def _data_exists(self, filename):\n",
    "        return os.path.exists(os.path.join(self.data_dir, filename))\n",
    "    \n",
    "    def fetch_nodes(self):\n",
    "        node_data = {}\n",
    "        node_types = self.graph.run(\"CALL db.labels()\").data()\n",
    "        for node_type in node_types:\n",
    "            label = node_type[\"label\"]\n",
    "            filename = f'nodes_{label}.json'\n",
    "            if self.use_file_storage and self._data_exists(filename):\n",
    "                node_data[label] = self._load_data(filename)\n",
    "            else:\n",
    "                query = f\"\"\"\n",
    "                    MATCH (n:{label})\n",
    "                    RETURN id(n) AS id, n.sbert_embedding AS embedding, n.name AS name\n",
    "                \"\"\"\n",
    "                results = self.graph.run(query).data()\n",
    "                print(label,len(results))\n",
    "                embeddings = [result['embedding'] for result in results]\n",
    "                names = [result['name'] for result in results]\n",
    "                node_data[label] = {'embeddings': embeddings, 'names': names}\n",
    "                if self.use_file_storage:\n",
    "                    self._save_data(node_data[label], filename)\n",
    "        return node_data\n",
    "\n",
    "\n",
    "    def fetch_relationships(self):\n",
    "        edge_data = {}\n",
    "        rel_types = self.graph.run(\"CALL db.relationshipTypes()\").data()\n",
    "        # This could be a class attribute\n",
    "        self.rel_type_mapping = {rel_type['relationshipType']: i for i, rel_type in enumerate(self.graph.run(\"CALL db.relationshipTypes()\").data())}\n",
    "\n",
    "        for rel_type in rel_types:\n",
    "            type_ = rel_type[\"relationshipType\"]\n",
    "            filename = f'rels_{type_}.json'\n",
    "            if self.use_file_storage and self._data_exists(filename):\n",
    "                edge_data[type_] = self._load_data(filename)\n",
    "            else:\n",
    "\n",
    "                query = f\"\"\"\n",
    "                    MATCH ()-[r:{type_}]->()\n",
    "                    RETURN id(startNode(r)) AS source, id(endNode(r)) AS target, r.weight AS weight\n",
    "                \"\"\"\n",
    "                filename = f'rels_{type_}.json'\n",
    "                results = self.graph.run(query).data()\n",
    "                print(rel_type,len(results))\n",
    "                edge_index = torch.tensor([(result['source'], result['target']) for result in results], dtype=torch.long).t().contiguous()\n",
    "                weights = torch.tensor([result['weight'] for result in results], dtype=torch.float)\n",
    "                rel_type_idx = torch.full((edge_index.size(1),), self.rel_type_mapping[type_], dtype=torch.long)\n",
    "                edge_data[type_] = {\n",
    "                                        'edge_index': edge_index.tolist(),  # Convert to list\n",
    "                                        'weights': weights.tolist(),  # Convert to list\n",
    "                                        'rel_type': rel_type_idx.tolist()  # Convert to list\n",
    "                                    }\n",
    "                if self.use_file_storage:\n",
    "                        self._save_data(edge_data[type_], filename)\n",
    "        return edge_data\n",
    "        \n",
    "    def to_pyg_hetero_data(self):\n",
    "        hetero_data = HeteroData()\n",
    "        \n",
    "        node_data = self.fetch_nodes()\n",
    "        for node_type, data in node_data.items():\n",
    "            hetero_data[node_type].x = torch.tensor(data['embeddings'], dtype=torch.float)  # Convert back to tensor\n",
    "            hetero_data[node_type].name = data['names']  # Names are already in the correct format (list of strings)\n",
    "        \n",
    "        edge_data = self.fetch_relationships()\n",
    "        for rel_type, data in edge_data.items():\n",
    "            edge_index = torch.tensor(data['edge_index'], dtype=torch.long).t().contiguous()\n",
    "            weights = torch.tensor(data['weights'], dtype=torch.float)\n",
    "            rel_type_idx = torch.tensor(data['rel_type'], dtype=torch.long)\n",
    "            hetero_data[rel_type].edge_index = edge_index\n",
    "            hetero_data[rel_type].edge_attr = torch.stack([weights, rel_type_idx], dim=1)  # Stack weights and relationship types\n",
    "\n",
    "        return hetero_data\n",
    "\n",
    "def convert_database(database):\n",
    "    neo4j_store = Neo4jHeteroGraphStore(uri=host, user=user, password=password, database=database, use_file_storage=True)\n",
    "    pyg_hetero_data = neo4j_store.to_pyg_hetero_data()\n",
    "    print(pyg_hetero_data.metadata())\n",
    "    return pyg_hetero_data\n",
    "    \n",
    "data =convert_database('ome-alerts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a7ac637-22a2-487b-9afe-7e75ad4cfbc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  User={\n",
       "    x=[49, 384],\n",
       "    name=[49],\n",
       "  },\n",
       "  User_section={\n",
       "    x=[693, 384],\n",
       "    name=[693],\n",
       "  },\n",
       "  Author_keywords={\n",
       "    x=[0],\n",
       "    name=[0],\n",
       "  },\n",
       "  Document_type={\n",
       "    x=[15, 384],\n",
       "    name=[15],\n",
       "  },\n",
       "  Company_txt_ss={\n",
       "    x=[1824, 384],\n",
       "    name=[1824],\n",
       "  },\n",
       "  Indication_MeSH_txt_ss={\n",
       "    x=[2465, 384],\n",
       "    name=[2465],\n",
       "  },\n",
       "  drug_OME_txt_ss={\n",
       "    x=[4929, 384],\n",
       "    name=[4929],\n",
       "  },\n",
       "  target_OME_txt_ss={\n",
       "    x=[7073, 384],\n",
       "    name=[7073],\n",
       "  },\n",
       "  Keyword={\n",
       "    x=[1054, 384],\n",
       "    name=[1054],\n",
       "  },\n",
       "  Alias={\n",
       "    x=[43330, 384],\n",
       "    name=[43330],\n",
       "  },\n",
       "  Article={\n",
       "    x=[20710, 384],\n",
       "    name=[20710],\n",
       "  },\n",
       "  Source={\n",
       "    x=[772, 384],\n",
       "    name=[772],\n",
       "  },\n",
       "  Language={\n",
       "    x=[2, 384],\n",
       "    name=[2],\n",
       "  },\n",
       "  disease={\n",
       "    x=[5989, 384],\n",
       "    name=[5989],\n",
       "  },\n",
       "  company={\n",
       "    x=[7772, 384],\n",
       "    name=[7772],\n",
       "  },\n",
       "  SENT_TO={\n",
       "    edge_index=[63278, 2],\n",
       "    edge_attr=[63278, 2],\n",
       "  },\n",
       "  TYPE={\n",
       "    edge_index=[20710, 2],\n",
       "    edge_attr=[20710, 2],\n",
       "  },\n",
       "  CONTAINS={\n",
       "    edge_index=[111931, 2],\n",
       "    edge_attr=[111931, 2],\n",
       "  },\n",
       "  LANGUAGE={\n",
       "    edge_index=[1725, 2],\n",
       "    edge_attr=[1725, 2],\n",
       "  },\n",
       "  SUB_SECTION={\n",
       "    edge_index=[672, 2],\n",
       "    edge_attr=[672, 2],\n",
       "  },\n",
       "  FROM_WEBSITE={\n",
       "    edge_index=[20567, 2],\n",
       "    edge_attr=[20567, 2],\n",
       "  },\n",
       "  SIMILAR={\n",
       "    edge_index=[160, 2],\n",
       "    edge_attr=[160, 2],\n",
       "  },\n",
       "  NDDv3={\n",
       "    edge_index=[261, 2],\n",
       "    edge_attr=[261, 2],\n",
       "  },\n",
       "  HAS_KEYWORD={\n",
       "    edge_index=[3790, 2],\n",
       "    edge_attr=[3790, 2],\n",
       "  },\n",
       "  HAS_ALIAS={\n",
       "    edge_index=[516016, 2],\n",
       "    edge_attr=[516016, 2],\n",
       "  },\n",
       "  NAME_MATCH={\n",
       "    edge_index=[1388, 2],\n",
       "    edge_attr=[1388, 2],\n",
       "  },\n",
       "  MAIN_DISEASE={\n",
       "    edge_index=[43007, 2],\n",
       "    edge_attr=[43007, 2],\n",
       "  },\n",
       "  MAIN_COMPANY={\n",
       "    edge_index=[37359, 2],\n",
       "    edge_attr=[37359, 2],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c13cd529-da39-4595-a036-0ebebd5a25c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node type: User\n",
      "  Features (x) available for node type 'User'\n",
      "Node type: User_section\n",
      "  Features (x) available for node type 'User_section'\n",
      "Node type: Author_keywords\n",
      "  Features (x) available for node type 'Author_keywords'\n",
      "Node type: Document_type\n",
      "  Features (x) available for node type 'Document_type'\n",
      "Node type: Company_txt_ss\n",
      "  Features (x) available for node type 'Company_txt_ss'\n",
      "Node type: Indication_MeSH_txt_ss\n",
      "  Features (x) available for node type 'Indication_MeSH_txt_ss'\n",
      "Node type: drug_OME_txt_ss\n",
      "  Features (x) available for node type 'drug_OME_txt_ss'\n",
      "Node type: target_OME_txt_ss\n",
      "  Features (x) available for node type 'target_OME_txt_ss'\n",
      "Node type: Keyword\n",
      "  Features (x) available for node type 'Keyword'\n",
      "Node type: Alias\n",
      "  Features (x) available for node type 'Alias'\n",
      "Node type: Article\n",
      "  Features (x) available for node type 'Article'\n",
      "Node type: Source\n",
      "  Features (x) available for node type 'Source'\n",
      "Node type: Language\n",
      "  Features (x) available for node type 'Language'\n",
      "Node type: disease\n",
      "  Features (x) available for node type 'disease'\n",
      "Node type: company\n",
      "  Features (x) available for node type 'company'\n",
      "Node type: SENT_TO\n",
      "  Features (x) NOT available for node type 'SENT_TO'\n",
      "Node type: TYPE\n",
      "  Features (x) NOT available for node type 'TYPE'\n",
      "Node type: CONTAINS\n",
      "  Features (x) NOT available for node type 'CONTAINS'\n",
      "Node type: LANGUAGE\n",
      "  Features (x) NOT available for node type 'LANGUAGE'\n",
      "Node type: SUB_SECTION\n",
      "  Features (x) NOT available for node type 'SUB_SECTION'\n",
      "Node type: FROM_WEBSITE\n",
      "  Features (x) NOT available for node type 'FROM_WEBSITE'\n",
      "Node type: SIMILAR\n",
      "  Features (x) NOT available for node type 'SIMILAR'\n",
      "Node type: NDDv3\n",
      "  Features (x) NOT available for node type 'NDDv3'\n",
      "Node type: HAS_KEYWORD\n",
      "  Features (x) NOT available for node type 'HAS_KEYWORD'\n",
      "Node type: HAS_ALIAS\n",
      "  Features (x) NOT available for node type 'HAS_ALIAS'\n",
      "Node type: NAME_MATCH\n",
      "  Features (x) NOT available for node type 'NAME_MATCH'\n",
      "Node type: MAIN_DISEASE\n",
      "  Features (x) NOT available for node type 'MAIN_DISEASE'\n",
      "Node type: MAIN_COMPANY\n",
      "  Features (x) NOT available for node type 'MAIN_COMPANY'\n"
     ]
    }
   ],
   "source": [
    "for node_type in data.node_types:\n",
    "    print(f\"Node type: {node_type}\")\n",
    "    if 'x' in data[node_type].keys():\n",
    "        print(f\"  Features (x) available for node type '{node_type}'\")\n",
    "    else:\n",
    "        print(f\"  Features (x) NOT available for node type '{node_type}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfbb9eca-cc92-4534-b09b-20d3b1219787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "\n",
    "class HeteroGraphInMemoryDataset(InMemoryDataset):\n",
    "    def __init__(self, root, hetero_data, transform=None, pre_transform=None):\n",
    "        self.hetero_data = hetero_data\n",
    "        super(HeteroGraphInMemoryDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        # No raw files, but we need to override this property\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        # Assuming we'll store everything in a single file for simplicity\n",
    "        return ['hetero_data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        # No download needed, data is already in memory\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        # Initialize empty dictionaries to hold node features and edge lists for all types\n",
    "        x_dict = {}\n",
    "        edge_index_dict = {}\n",
    "        edge_attr_dict = {}\n",
    "    \n",
    "        # Process node features\n",
    "        for node_type in self.hetero_data.node_types:\n",
    "            if 'x' in self.hetero_data[node_type].keys():\n",
    "                x_dict[node_type] = self.hetero_data[node_type].x\n",
    "    \n",
    "        # Process edges\n",
    "        for rel_type in self.hetero_data.edge_types:\n",
    "            edge_index = self.hetero_data[rel_type].edge_index\n",
    "            if edge_index.size(0) == 0:\n",
    "                continue  # Skip if no edges of this type\n",
    "            edge_index_dict[rel_type] = edge_index\n",
    "            # Assuming edge_attr exists and corresponds to edge weights\n",
    "            if 'edge_attr' in self.hetero_data[rel_type]:\n",
    "                edge_attr_dict[rel_type] = self.hetero_data[rel_type].edge_attr\n",
    "    \n",
    "        # Create a single Data object for the entire heterogeneous graph\n",
    "        data = Data(x_dict=x_dict, edge_index_dict=edge_index_dict, edge_attr_dict=edge_attr_dict)\n",
    "    \n",
    "        # If you have pre_transforms, apply them\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "    \n",
    "        # Save the processed data\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        # Load a single graph\n",
    "        data = torch.load(self.processed_paths[idx])\n",
    "        return data\n",
    "   \n",
    "# Assuming `hetero_data` is your HeteroData object from Neo4j\n",
    "root_dir = '/root/Downloads/Jupyterlab/Ankur_Notebooks/Exphormer/data/'\n",
    "dataset = HeteroGraphInMemoryDataset(root=root_dir, hetero_data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e409b073-af69-4f88-893b-35c329acc11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroGraphInMemoryDataset()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".KG_env",
   "language": "python",
   "name": ".kg_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
